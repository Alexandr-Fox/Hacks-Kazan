{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7e1dbdd10824935"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import itertools\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:54:04.127525300Z",
     "start_time": "2023-09-08T13:54:03.510431700Z"
    }
   },
   "id": "18b590e81e359dc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a26d85a1f6f3b4aa"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Это пример предложения для эмбеддингов\"\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)])\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids)\n",
    "bert_embeddings = model_output[0]\n",
    "print(bert_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:19:42.723586100Z",
     "start_time": "2023-09-08T13:19:42.673919Z"
    }
   },
   "id": "236b41ea2f70af10"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Global vars\n",
    "_verbose = 0\n",
    "_batch_size = 1\n",
    "# bert_path = \"../\"\n",
    "# Init word embedding models once\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/deberta-v1-base\")\n",
    "model = AutoModel.from_pretrained(\"deepvk/deberta-v1-base\", output_hidden_states=True)\n",
    "\n",
    "# Helper func 1\n",
    "def get_word_idx(sent: str, word: str) -> int:\n",
    "    \"\"\"split sentences and add index to each word. Each word has its own index based on when it was added to the list first\n",
    "    Args:\n",
    "        sent (str): sentence in string\n",
    "        word (str): word in string\n",
    "    Returns:\n",
    "        int: output the index of where the word correspond to in each sentence input\n",
    "    \"\"\"\n",
    "    return sent.lower().split(\" \").index(word)\n",
    "\n",
    "\n",
    "# Helper func 2\n",
    "def get_hidden_states(sent, tokenizer, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "       Select only those subword token outputs that belong to our word of interest\n",
    "       and average them.\n",
    "    Args:\n",
    "        sent (str): Input sentence\n",
    "        tokenizer : Tokenizer function\n",
    "        model: bert model\n",
    "        layers : last 4 model of model\n",
    "    Returns:\n",
    "        output: tensor torch\n",
    "    \"\"\"\n",
    "    # encode without adding [CLS] and [SEP] tokens\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    return output\n",
    "\n",
    "\n",
    "# Helper func 3\n",
    "def chunking(max_len, sent):\n",
    "    \"\"\"because the embedding function is trained on dim 512, so we have to limit the size of the sentences using max_len so the final chunked sentences wont exceed length 512\n",
    "    Args:\n",
    "        max_len (int): maximum number of tokens for each chunk\n",
    "        sent (str): input sentence\n",
    "    Returns:\n",
    "        sent_chunk (List(str)): list of chunked sentences\n",
    "    \"\"\"\n",
    "    tokenized_text = sent.lower().split(\" \")\n",
    "    # using list comprehension\n",
    "    final = [\n",
    "        tokenized_text[i * max_len : (i + 1) * max_len]\n",
    "        for i in range((len(tokenized_text) + max_len - 1) // max_len)\n",
    "    ]\n",
    "\n",
    "    # join back to sentences for each of the chunks\n",
    "    sent_chunk = []\n",
    "    for item in final:\n",
    "        # make sure the len(items) > 1 or else some of the embeddings will appear as len 1 instead of 768.\n",
    "        assert len(item) > 1\n",
    "        sent_chunk.append(\" \".join(item))\n",
    "    return sent_chunk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:54:08.997188100Z",
     "start_time": "2023-09-08T13:54:06.293839200Z"
    }
   },
   "id": "3dd53eab8eb4b567"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def main_avg(sent: str, layers=None, chunk_size=300):\n",
    "    \"\"\"Gives the average word embedding per sentence\n",
    "\n",
    "    Args:\n",
    "        sent (str): The input sentence\n",
    "\n",
    "    Returns:\n",
    "        torch tensor: word embedding per sentence, dim = 768\n",
    "    \"\"\"\n",
    "    # change all standard form numbers to decimal\n",
    "    np.set_printoptions(formatter={\"float_kind\": \"{:f}\".format})\n",
    "\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "    global tokenizer\n",
    "    global model\n",
    "\n",
    "    # chunking\n",
    "    chunked_tokens = chunking(chunk_size, sent)  # helper func 3\n",
    "\n",
    "    # initialise a outside chunk\n",
    "    word_embedding_avg_collective = []\n",
    "    # for each chunked token, we embed them separately\n",
    "    for item in chunked_tokens:\n",
    "        # adding tensors\n",
    "        word_embedding_torch = get_hidden_states(\n",
    "            item, tokenizer, model, layers\n",
    "        )  # helper fun 2\n",
    "\n",
    "        # convert torch tensor to numpy array\n",
    "        word_embedding_avg_np = word_embedding_torch.cpu().detach().numpy()\n",
    "        word_embedding_avg_chunks = np.mean(word_embedding_avg_np, axis=0)\n",
    "        word_embedding_avg_collective.append(word_embedding_avg_chunks)\n",
    "    word_embedding_avg = np.mean(word_embedding_avg_collective, axis=0)\n",
    "    assert len(word_embedding_avg) == 768\n",
    "    return word_embedding_avg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:54:16.650851600Z",
     "start_time": "2023-09-08T13:54:16.612668500Z"
    }
   },
   "id": "d3422d367ccaab00"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.591490 -0.380062 0.935977 -1.466756 0.808263 0.013020 -0.496918\n",
      " -10.941804 -1.330297 -0.217808 0.064943 0.358653 -0.874851 -0.792649\n",
      " 0.958106 -0.463012 0.428570 -0.055375 -1.106878 1.328627 0.185171\n",
      " -0.444248 -0.940792 -1.615614 0.415217 0.418257 -0.478086 0.272854\n",
      " 1.911883 1.071831 0.520323 -0.097729 0.761777 -1.053714 -0.428464\n",
      " 0.723403 -0.143116 -0.338280 0.087615 -0.608163 -0.684337 0.241025\n",
      " 0.205047 0.160684 0.560690 -0.509371 0.228526 0.139592 -0.401513 0.466110\n",
      " 0.504656 0.647467 0.920083 0.063520 0.490110 0.844734 -0.185446 -0.947576\n",
      " -0.017977 -1.306925 0.441733 0.623836 -0.391002 0.676817 -0.097150\n",
      " -0.819193 -0.442789 0.830710 -0.973627 0.454027 0.652821 0.430550\n",
      " -1.082851 0.571368 -0.128744 -2.208430 0.294621 0.664620 -0.327884\n",
      " -1.605323 0.538414 -0.597383 0.371870 0.279613 0.642930 -0.065847\n",
      " 0.761038 -2.042186 0.613059 -0.712953 0.242563 -0.091222 0.557234\n",
      " -0.637208 1.312360 0.338818 -0.652879 -0.948241 -0.194665 -1.251956\n",
      " 0.374977 0.655686 -0.814976 1.093540 0.275661 -0.434877 0.799851 0.436545\n",
      " 0.331112 -0.154190 -0.064439 0.683185 -0.810705 -0.514997 -0.641498\n",
      " 1.036528 -0.756197 -0.307323 0.849017 -0.093321 -0.648550 -0.076380\n",
      " -0.231395 -0.411101 0.426601 -1.319116 0.121442 -0.185036 -0.193992\n",
      " -0.163580 -1.385102 -0.258679 0.145670 -0.989605 -1.648415 -0.427949\n",
      " -0.523123 -1.302229 -0.247345 -0.762565 -0.327728 -1.843610 0.368365\n",
      " -0.381433 -0.727304 0.247077 -0.966328 0.337061 0.107018 -1.407529\n",
      " -1.131662 0.160820 -0.921373 0.267202 1.008067 -0.364890 0.475260\n",
      " -0.233755 -1.563186 -0.844377 -1.284914 0.015880 0.584051 -3.540938\n",
      " -0.900682 -0.866720 0.693910 0.060447 -0.358970 -2.226747 0.796712\n",
      " -2.226169 -0.157512 1.260562 0.179980 -0.075747 0.468939 0.165949\n",
      " -0.012767 0.966594 -0.397216 0.389130 -1.523324 0.675201 1.925663\n",
      " 0.184308 0.197461 0.745295 0.758608 0.152717 0.021190 0.124000 -0.853189\n",
      " -0.857816 -0.533619 0.889172 0.801507 -0.529774 0.452916 -0.652607\n",
      " -0.845321 0.501900 -0.533621 -0.624816 0.064392 0.797501 0.077871\n",
      " -0.012772 -0.429501 -0.010858 -1.035905 -0.467721 0.771246 0.176055\n",
      " -0.544399 -1.874847 0.092236 -0.307849 -0.028699 0.772141 0.285190\n",
      " 0.116071 0.577023 0.787611 0.881348 -0.654006 0.817000 -0.659693 0.131542\n",
      " 0.516985 2.296693 -0.740259 0.207626 -0.884890 2.730588 -2.155676\n",
      " 0.331391 0.180131 -0.335525 0.493036 -1.175914 0.413824 0.466101 0.699789\n",
      " -0.166775 -0.150245 1.107712 0.309418 -0.746234 0.580521 -0.563435\n",
      " -0.142583 1.376444 0.637739 -0.237233 -0.159915 0.888471 1.313988\n",
      " -0.069997 0.477879 -0.167037 -1.586865 -0.052752 6.478464 -0.515442\n",
      " 1.117170 1.056305 -6.588128 -0.695664 0.299232 0.136519 -0.315782\n",
      " 0.945438 -0.432161 -0.246656 -0.850771 0.033049 -0.417221 0.515217\n",
      " -0.726830 -0.210618 0.493932 0.245159 0.311303 0.135456 0.557654\n",
      " -0.952864 -0.621319 0.658734 -0.007559 0.566306 0.314969 -0.368991\n",
      " -0.231252 0.812328 -0.606777 0.257214 -0.348485 0.472132 -0.570235\n",
      " -1.624588 0.708929 0.021913 -0.217777 0.663093 0.710031 -1.044935\n",
      " 0.871514 0.475394 0.427358 0.395790 0.687102 -2.273424 -0.097562\n",
      " -1.495641 -0.005370 -0.823747 4.094385 -0.927514 -0.380749 -0.226805\n",
      " 1.027413 0.482106 -1.332083 0.105739 -0.658376 0.358061 -0.044077\n",
      " -0.862482 0.726279 -0.641625 -1.686469 -0.441331 1.830642 0.662793\n",
      " 0.862602 0.440870 0.793421 -0.530335 0.305631 -0.188588 0.287336\n",
      " -0.856607 -0.306073 -0.179040 0.673299 0.422070 -0.306201 0.263292\n",
      " 0.236523 -0.055414 1.605794 -0.119009 0.113210 0.835979 -2.759481\n",
      " 0.711631 0.560389 0.123843 1.017719 0.321394 -0.284560 0.079375 0.070225\n",
      " 0.850978 -0.527778 -1.632019 0.490142 -0.353774 0.799611 -0.065044\n",
      " 1.268505 0.217400 1.191402 -0.606658 -0.532729 2.751628 0.382544\n",
      " -1.381985 0.238205 0.660931 0.698867 -0.034211 0.314097 0.307277\n",
      " -0.654982 0.772754 -1.561761 0.735667 0.245586 0.161843 -0.500545\n",
      " 0.143481 0.054933 0.054119 0.881418 -0.139353 -0.131831 -0.506762\n",
      " 0.009269 0.085002 -1.562837 -0.435328 -0.464169 -1.117571 -1.112789\n",
      " -0.369060 -0.780775 -1.402363 -0.535728 0.476404 -1.598469 0.006075\n",
      " 0.931525 1.324085 -0.076485 0.527752 0.362116 1.052310 -0.312083\n",
      " -0.316460 -1.237960 0.025563 0.173634 0.341926 0.625791 0.086581 0.634377\n",
      " -0.741279 -0.697116 0.328607 0.296364 -0.239465 -0.726043 0.063969\n",
      " -0.960980 0.681630 -0.014663 -0.003517 -1.586464 -1.031765 -0.460815\n",
      " 0.893344 -0.854343 0.188106 -0.391718 0.265889 0.017601 -1.104696\n",
      " -0.917848 -0.036274 -0.003196 0.049214 0.092412 0.714670 -0.282782\n",
      " -0.821935 -0.102716 -0.605860 -0.308993 -2.148945 0.401448 1.214170\n",
      " -1.134354 -0.481457 -1.141657 -1.115421 -0.726798 -0.622890 -2.194760\n",
      " 0.286597 -1.599772 -0.478175 -0.955140 -0.153137 -0.802407 0.988162\n",
      " 0.154832 -0.319759 -0.729635 0.835813 -0.492578 0.239744 -1.053154\n",
      " -0.747652 -2.039427 -0.278604 -0.925117 -0.078125 -0.974131 -0.519034\n",
      " -0.355554 0.150508 -0.406610 -0.068915 0.180263 0.324672 0.453978\n",
      " 0.454246 -0.867682 -0.190980 0.269693 -0.271454 0.912657 0.103823\n",
      " 0.307059 -1.374521 0.554957 -0.474717 0.896147 0.807675 -0.441370\n",
      " -0.153097 -0.648660 -0.239857 -0.092127 0.605249 0.281161 0.068072\n",
      " -0.389413 0.611597 0.692463 -0.068516 -0.589099 -1.016495 -0.063375\n",
      " 0.353187 0.539829 -0.578933 0.327212 0.077130 -0.549748 -0.606935\n",
      " -0.326489 1.145448 0.393619 0.237614 0.150759 -0.557753 1.230228 0.413217\n",
      " -0.872630 -0.409493 0.753293 -0.195555 0.408531 -0.984186 -0.292008\n",
      " 0.339674 -0.656334 -0.586297 0.914364 -0.310436 -1.387531 0.561758\n",
      " 0.456072 -0.679172 0.114238 0.383893 0.557668 -0.454786 0.581841 0.573464\n",
      " 0.516969 -3.839598 -0.833559 0.384220 -0.430840 -0.122114 -0.174472\n",
      " 7.983678 0.330845 -0.238291 -1.389985 -0.103094 -1.082525 -0.319444\n",
      " 0.045206 0.228519 0.382480 0.839529 0.134140 1.038629 0.214995 -0.167866\n",
      " -1.009598 0.073558 0.440945 -0.265186 -0.211096 0.494565 -0.688258\n",
      " -0.223132 -0.620741 -0.125445 -0.883626 -0.725299 0.125784 -1.376121\n",
      " 0.701351 -1.034598 -0.148650 -0.407448 0.562717 -0.432463 -0.198412\n",
      " 2.091081 -1.977719 0.007081 -0.022335 0.032590 0.737940 0.385624\n",
      " -0.271349 1.162891 -0.306414 0.570340 -0.486251 -0.277684 0.432620\n",
      " -0.051138 0.250631 1.899970 -0.613259 0.414545 -1.022066 0.722668\n",
      " -0.545447 0.362067 -1.052211 0.505089 -0.141117 -1.225744 -1.131228\n",
      " -1.067759 0.088514 -0.075447 0.800374 0.383098 -0.058873 -0.374116\n",
      " 0.864913 0.151412 2.153195 -0.527252 -0.222621 -0.524988 -0.427134\n",
      " -0.998631 0.686396 -0.397306 -0.441352 0.881444 0.637411 0.868833\n",
      " -0.796594 1.110186 0.493808 0.035346 -0.558807 -0.507391 -0.739466\n",
      " 0.024998 -0.443870 0.056420 0.134660 -0.837161 0.023921 0.210717\n",
      " -0.542305 -0.414848 -1.798061 -0.295332 0.015524 -0.222783 -1.045770\n",
      " 0.316455 -0.421959 0.199647 0.342275 -0.144409 -0.062183 0.138633\n",
      " -0.248371 -0.272943 2.468730 0.117921 -0.109812 -0.044787 0.916405\n",
      " -0.111430 -0.482714 -0.518361 -0.111879 0.428698 1.072396 0.413844\n",
      " 0.336189 0.873772 -0.435166 0.429389 -0.015516 0.289009 -0.282733\n",
      " -0.766428 0.147169 -0.914896 1.139497 0.094646 0.216996 -0.453440\n",
      " 1.059987 -0.153933 -0.230461 -0.275811 0.298245 -0.334915 0.629155\n",
      " 0.126027 -0.447211 0.005156 -0.564744 -1.050846 -0.612221 -0.269426\n",
      " -0.232977 -0.070532 -0.186512 0.276878 -0.479083 1.037466 -0.545933\n",
      " -0.192452 -0.679545 0.180149 -0.730049 0.940794 1.529940 -0.159396\n",
      " 0.063653 0.776621 -0.213225 -0.815605 -1.097114 -0.591458 0.430954\n",
      " -0.562522 -1.014297 0.372377 0.183094 -0.183905 -0.040535 -0.180199\n",
      " -0.601129 -1.649462 -0.783401 0.377163 -0.670440 0.193879 1.056812\n",
      " -0.876236 0.649077 0.352652 -0.015580 -0.167080 0.383688 -0.594069\n",
      " 0.498366]\n"
     ]
    }
   ],
   "source": [
    "avg_input = main_avg(\"Это пример предложения для эмбеддингов\", layers=None, chunk_size=300)\n",
    "print(avg_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:55:42.257991200Z",
     "start_time": "2023-09-08T13:55:42.128873500Z"
    }
   },
   "id": "c53ebceb730c3f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "64989322976ce4bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
