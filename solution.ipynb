{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "pickled_model = pickle.load(open('final_model_jaccard_2.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "def get_scores(X, y, x_predict, return_names=False, return_only_names=False):\n",
    "    if return_only_names is True and return_names is False:\n",
    "        raise ValueError(\"return_only_names cannot be True if return_names is False\")\n",
    "    clf = LogisticRegression(max_iter=3000).fit(X.to_numpy(), y)\n",
    "    scores = clf.predict_proba([x_predict])\n",
    "    if return_names is False:\n",
    "        return scores\n",
    "    else:\n",
    "        classes = clf.classes_\n",
    "        scores = scores.tolist()[0]\n",
    "        classes = [(classes[i], scores[i]) for i in range(10)]\n",
    "        classes.sort(key=lambda x: x[1], reverse=True)\n",
    "        if return_only_names is True:\n",
    "            classes = [i[0] for i in classes]\n",
    "            print('результаты', classes)\n",
    "        return classes\n",
    "\n",
    "\n",
    "def get_scores_catboost(X, y, x_predict, return_names=False, return_only_names=False):\n",
    "    if return_only_names is True and return_names is False:\n",
    "        raise ValueError(\"return_only_names cannot be True if return_names is False\")\n",
    "    clf = CatBoostClassifier(iterations=1000, learning_rate=0.01, logging_level='Silent').fit(X, y)\n",
    "    scores = clf.predict_proba([x_predict])\n",
    "    if return_names is False:\n",
    "        return scores\n",
    "    else:\n",
    "        classes = clf.classes_\n",
    "        scores = scores.tolist()[0]\n",
    "        classes = [(classes[i], scores[i]) for i in range(10)]\n",
    "        classes.sort(key=lambda x: x[1], reverse=True)\n",
    "        if return_only_names is True:\n",
    "            classes = [i[0] for i in classes]\n",
    "            print(classes)\n",
    "        return classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pynndescent import NNDescent\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def vid_long(videos: DataFrame, user_id: int, users: DataFrame) -> DataFrame:\n",
    "    tqdm.pandas()\n",
    "    hist = users[users[\"user_id\"] == user_id]\n",
    "    hist = pd.merge(hist, videos, on='item_id').drop(columns=[\"user_id\"], axis=1)\n",
    "    for i in tqdm(range(100)):\n",
    "        hist[f\"v_title_{i}\"] = hist.progress_apply(lambda row: row[f\"v_title_{i}\"] * 2\n",
    "        if ((row[\"watch_time\"] / (row[\"duration\"] / 1000)) > 0.25\n",
    "            if (row[\"duration\"] / 1000) > 300\n",
    "            else row[\"watch_time\"] > 30)\n",
    "        else 1, axis=1)\n",
    "        hist[f\"v_title_{i}\"] = preprocessing.minmax_scale(hist[f\"v_title_{i}\"].T).T\n",
    "    return hist.drop(columns=[\"watch_time\", \"duration\"], axis=1)\n",
    "\n",
    "\n",
    "def like(videos: pd.DataFrame, user_id: int, emotions: pd.DataFrame) -> pd.DataFrame:\n",
    "    emotions = emotions[[\"user_id\", \"item_id\", \"type\"]]\n",
    "    emotions = emotions[emotions[\"user_id\"] == user_id]\n",
    "    emotions = emotions[[\"item_id\", \"type\"]]\n",
    "    # hist = users[users[\"user_id\"] == user_id]\n",
    "    # hist = pd.merge(hist, videos, on='item_id').drop(columns=[\"user_id\"], axis=1)\n",
    "    # hist = pd.merge(hist, emotions, left_on='item_id', right_on=\"C3\")\n",
    "    tqdm.pandas()\n",
    "    new_videos = videos.merge(emotions, on=\"item_id\")\n",
    "    if len(new_videos) != 0:\n",
    "        videos = new_videos\n",
    "        for i in tqdm(range(100)):\n",
    "            videos[f\"vid_title_{i}\"] = videos.progress_apply(\n",
    "                lambda row: row[f\"vid_title_{i}\"] * 2 if row[\"type\"] == \"pos_emotions\"\n",
    "                else (0.5 if row[\"type\"] == \"neg_emotions\" else 1), axis=1).to_numpy()\n",
    "            videos[f\"vid_title_{i}\"] = preprocessing.minmax_scale(np.array([videos[f\"vid_title_{i}\"]]).T).T\n",
    "            videos[f\"vid_descr_{i}\"] = preprocessing.minmax_scale(np.array([videos.progress_apply(\n",
    "                lambda row: row[f\"vid_descr_{i}\"] * 2 if row[\"type\"] == \"pos_emotions\"\n",
    "                else (0.5 if row[\"type\"] == \"neg_emotions\" else 1), axis=1).to_numpy()]).T).T\n",
    "\n",
    "            # normalize(np.array([videos.progress_apply(\n",
    "            #         lambda row: row[f\"vtitle{i}\"] * 2 if row[\"C4\"] == \"pos_emotions\"\n",
    "            #         else (0.5 if row[\"C4\"] == \"neg_emotions\" else 1), axis=1).to_numpy()]))[0]\n",
    "        return videos.drop(columns=[\"type\"], axis=1)\n",
    "    else:\n",
    "        return videos\n",
    "    # return videos.drop(columns=[\"type\"], axis=1)\n",
    "\n",
    "\n",
    "def make_vector(data: DataFrame, users: DataFrame, emotions: DataFrame, user_id: str):\n",
    "    # print(\"история просмотра\", video_ids)\n",
    "\n",
    "    video_ids = users.query(\"user_id == @user_id\")[\"item_id\"].values\n",
    "    df = data.loc[data[\"item_id\"].isin(video_ids)]\n",
    "\n",
    "    # vid_long(data, user_id, users)\n",
    "    # likes = like(df, user_id, emotions)\n",
    "\n",
    "    # TODO add weights\n",
    "    return df._get_numeric_data().mean()\n",
    "    # return np.mean(df.drop(['item_id'], axis=1).values, axis=0)\n",
    "    # return np.average(df.drop(['item_id', 'video_description', 'video_title', 'CTR_10days_01_08', 'author_title', 'tv_title', 'CTR_10days_21_07', 'CTR_10days_10_08', 'CTR_10days_21_08', 'tv_sub', 'season', 'publicated', 'category_title'], axis=1).values, axis=0)\n",
    "    # return np.mean(likes.drop(['item_id', 'video_description', 'video_title', 'ctr.CTR_10days_01_08', 'ctr.CTR_10days_21_07', 'ctr.CTR_10days_10_08', 'ctr.CTR_10days_21_08', 'tv_sub', 'season', 'publicated', 'category_title'], axis=1).values, axis=0)\n",
    "\n",
    "\n",
    "def get_video_corpus(data: DataFrame, vector: np.ndarray):\n",
    "    df = data.drop('item_id', axis=1).columns.to_list()\n",
    "    top100nearest = pickled_model.query(pd.DataFrame([vector], columns=df), k=10)\n",
    "    vids_indices = top100nearest[0][0]\n",
    "    res_data = data.iloc[vids_indices]\n",
    "    return res_data.drop(['item_id'], axis=1).values, res_data['item_id'].values\n",
    "    # return res_data.drop(['item_id', 'video_description', 'video_title', 'CTR_10days_01_08','author_title', 'tv_title', 'CTR_10days_21_07', 'CTR_10days_10_08', 'CTR_10days_21_08', 'tv_sub', 'season', 'publicated', 'category_title'], axis=1), res_data['item_id'].values\n",
    "\n",
    "\n",
    "def get_10_category(train_hist: pd.DataFrame, new_hist: pd.DataFrame = None) -> list:\n",
    "    if new_hist is not None:\n",
    "        train_hist = train_hist.append(new_hist, ignore_index=True)\n",
    "    cat_columns = [col for col in tqdm(train_hist.columns) if col.startswith('cat')]\n",
    "    cat_data = train_hist[cat_columns]\n",
    "\n",
    "    # подсчет количества 1 в каждом столбце\n",
    "    counts = cat_data.sum()\n",
    "\n",
    "    # сортировка столбцов по убыванию количества 1 и вывод первых 10\n",
    "    top_columns = counts.sort_values(ascending=False)[:10]\n",
    "    return top_columns\n",
    "\n",
    "\n",
    "def get_top_videos_in_cat(cat_name, train_hist: pd.DataFrame, new_hist: pd.DataFrame = None) -> str:\n",
    "    if new_hist is not None:\n",
    "        train_hist = train_hist.append(new_hist, ignore_index=True)\n",
    "    cat_data = train_hist.loc[train_hist[cat_name] == 1]\n",
    "    counts = cat_data['item_id'].value_counts()\n",
    "    return counts.index[0]\n",
    "\n",
    "\n",
    "def get_top_videos(train_hist: pd.DataFrame, new_hist: pd.DataFrame = None) -> list:\n",
    "    top_category = get_10_category(train_hist, new_hist)\n",
    "    return [get_top_videos_in_cat(i, train_hist, new_hist) for i in tqdm(top_category)]\n",
    "\n",
    "\n",
    "def get_popular() -> np.ndarray:\n",
    "    popular = pd.read_csv(\"top_cat.csv\")\n",
    "    popular = data[\"item_id\"].to_numpy()\n",
    "    popular = np.reshape(popular, (10, 10))\n",
    "    result = []\n",
    "    for i in range(10):\n",
    "        result.append(popular[i][random.randint(0, 9)])\n",
    "    result = np.random.permutation(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_user(user_id):\n",
    "    user_items = users.loc[users[\"user_id\"] == user_id, \"item_id\"].values\n",
    "    if len(user_items) == 0:\n",
    "        return get_popular()\n",
    "    x_predict = make_vector(data, user_items, users, emotions)\n",
    "    if len(x_predict) == 0:\n",
    "        return get_popular()\n",
    "    corpus, target = get_video_corpus(data, x_predict)\n",
    "    return target\n",
    "\n",
    "\n",
    "def create_submission_file(path: str, data: DataFrame, users: DataFrame, emotions: DataFrame):\n",
    "    test_file = pd.read_csv(path)\n",
    "    user_ids = test_file[\"user_id\"].values\n",
    "    preds = []\n",
    "    for idx, user_id in enumerate(tqdm(user_ids)):\n",
    "        # user_items = users.query(\"user_id == @user_id\")[\"item_id\"].values\n",
    "        x_predict = make_vector(data, users, emotions, user_id)\n",
    "        if len(x_predict) == 0:\n",
    "            preds.append(get_popular)\n",
    "            break\n",
    "        corpus, target = get_video_corpus(data, x_predict)\n",
    "        preds.append(target)\n",
    "\n",
    "        # if idx % 20 == 0:\n",
    "        #     submission = pd.DataFrame({\"user_id\": user_ids[:idx+1], \"recs\": preds})\n",
    "        #     submission.to_csv(f\"submission_{idx}.csv\", index=False)\n",
    "\n",
    "    submission = pd.DataFrame({\"user_id\": user_ids, \"recs\": preds})\n",
    "    submission.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/wf1rpzp51jj_bp2lz71rs3q80000gn/T/ipykernel_784/257456476.py:1: DtypeWarning: Columns (252) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"final_df_2.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"final_df_2.csv\")\n",
    "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "data.drop(['video_description', 'video_title', 'ctr.CTR_10days_01_08', 'author_title', 'tv_title', 'ctr.CTR_10days_21_07', 'ctr.CTR_10days_10_08', 'ctr.CTR_10days_21_08', 'tv_sub', 'season', 'publicated', 'category_title'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "users = pq.read_table('train_dataset_RUTUBE/player_starts_train.parquet').to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "emotions = pd.read_csv(\"train_dataset_RUTUBE/emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "neighbors = pickled_model.query(test.drop(['item_id'], axis=1).head(10), k=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_submission_file(\"train_dataset_RUTUBE/sample_submission.csv\", data, users, emotions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}